{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProiectPP_NewD _.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SlL29H14f0r"
      },
      "source": [
        "# **Probabilistic Programming Project**\r\n",
        "\r\n",
        "Ghadamiyan Lida\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "The purpose of this project is to implement the LDA algorithm using PyMC, following the indications from the course.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeutskVnz-kN"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "corpus = [\"basketball team sport team player compete objective shoot basketball\", \r\n",
        "          \"player basketball team point game win\", \r\n",
        "          \"center general tall basketball player tall basketball player forward\",\r\n",
        "          \"friends proffesional basketball player like dennis rodman\",\r\n",
        "          \r\n",
        "          \"art diverse range human activiti involv creat visual audit perform artifact\",\r\n",
        "          \"activities relat production work art include art criticism history art\",\r\n",
        "          \"three classical branche visual art paint sculpture architecture\",\r\n",
        "          \"music theatre film dance performing art literature includ broader definition art\"]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWJz13Cb4X70"
      },
      "source": [
        "##**Data Preprocessing**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "* Tokenization. \r\n",
        "* Stopwords removal - done by CountVectorizer in feature extraction. \r\n",
        "* Lowercase the words. \r\n",
        "* Remove punctuation.\r\n",
        "* Remove short words.\r\n",
        "* Lematization - reducing words to their meaningful base form.\r\n",
        "* Stemming â€” reducing words to their base form.\r\n",
        "* **Feature Extraction**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFETU3Dv021a",
        "outputId": "519ade5e-ad1d-4dcd-d7a7-0ad7a2d8e7f2"
      },
      "source": [
        "import string\r\n",
        "import nltk\r\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "stemmer = SnowballStemmer('english')\r\n",
        "\r\n",
        "def preprocessing(corpus):\r\n",
        "\r\n",
        "    df = pd.DataFrame({'doc':corpus})     # Converting to data frame\r\n",
        "\r\n",
        "    data = []\r\n",
        "    for i in range(0, len(df.index)):\r\n",
        "\r\n",
        "        table = str.maketrans(dict.fromkeys(string.punctuation))                    # Punctuation removal\r\n",
        "        words = (df.doc[i].translate(table)).lower() \r\n",
        "\r\n",
        "        words = nltk.word_tokenize(words) # Tokenization\r\n",
        "\r\n",
        "        words_ = []\r\n",
        "        for word in words:\r\n",
        "            if len(word) > 2:                                                       # Short words removal\r\n",
        "                word1 = stemmer.stem(WordNetLemmatizer().lemmatize(word, pos='v'))  # Lemmatization and stemming           \r\n",
        "                words_.append(word1)\r\n",
        "        data.append(words_)\r\n",
        "\r\n",
        "    corpus = pd.DataFrame({'doc':data})                                             # Saving as dataframe\r\n",
        "    return corpus, data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COEhAUgr2Vlt"
      },
      "source": [
        "data, data_ = preprocessing(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J-icJ_64c1D"
      },
      "source": [
        "# **Feature Extraction**\r\n",
        "\r\n",
        "* Bag of Words - CountVectorizer is used for building a dictionary of features, and also for tokenizing and filtering stopwords. \r\n",
        "* Frequencies - We compute the term frequencies by  dividing the number of occurrences given by CountVectorizer by the total number of words.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5miitznn2lWY"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "def feat_extraction(data):\r\n",
        "    data = data['doc'].astype(str).values.tolist() # Converting to list of list\r\n",
        "    count_vect =  CountVectorizer()\r\n",
        "    occurrence = count_vect.fit_transform(data)\r\n",
        "\r\n",
        "    return count_vect, occurrence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMLhj1dC2rTG",
        "outputId": "429b2ae7-dd03-4bd6-d1e3-904a6f8acc1e"
      },
      "source": [
        "voc, occ = feat_extraction(data)\r\n",
        "\r\n",
        "#print(voc.get_feature_names())\r\n",
        "#print(voc.vocabulary_)\r\n",
        "\r\n",
        "vocab_ = {v : k for k, v in voc.vocabulary_.items()}\r\n",
        "print(vocab_)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{5: 'basketbal', 43: 'team', 41: 'sport', 32: 'player', 10: 'compet', 29: 'object', 40: 'shoot', 33: 'point', 20: 'game', 47: 'win', 8: 'center', 21: 'general', 42: 'tall', 18: 'forward', 19: 'friend', 35: 'proffesion', 26: 'like', 15: 'denni', 38: 'rodman', 2: 'art', 16: 'divers', 36: 'rang', 23: 'human', 0: 'activ', 25: 'involv', 11: 'creat', 46: 'visual', 4: 'audit', 31: 'perform', 3: 'artifact', 37: 'relat', 34: 'product', 48: 'work', 24: 'includ', 12: 'critic', 22: 'histori', 45: 'three', 9: 'classic', 6: 'branch', 30: 'paint', 39: 'sculptur', 1: 'architectur', 28: 'music', 44: 'theatr', 17: 'film', 13: 'danc', 27: 'literatur', 7: 'broader', 14: 'definit'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6pzDyFoqHEJ"
      },
      "source": [
        "def frec(occ): \r\n",
        "    data_occ= []\r\n",
        "    for doc in occ.toarray():\r\n",
        "        words = []\r\n",
        "        for poz, n in enumerate(doc):\r\n",
        "            for j in range(n):\r\n",
        "                words.append(poz)\r\n",
        "        data_occ.append(words)\r\n",
        "\r\n",
        "\r\n",
        "    data_tf = []\r\n",
        "    for doc in data_occ:\r\n",
        "        doc_tf = []\r\n",
        "        for w in doc:\r\n",
        "            doc_tf.append(w / len(vocab_))\r\n",
        "        data_tf.append(doc_tf)\r\n",
        "    return data_tf, data_occ\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq-3cdDm1UDU"
      },
      "source": [
        "After I tried both with data and data_tf, I concluded that working with frequencies indead of occurences gives better resoults."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB3-d0d71YLJ",
        "outputId": "c0069bfe-1048-4011-db70-77792d263624"
      },
      "source": [
        "data, data_occ = frec(occ)\r\n",
        "print(data_occ)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 5, 10, 29, 32, 40, 41, 43, 43], [5, 20, 32, 33, 43, 47], [5, 5, 8, 18, 21, 32, 32, 42, 42], [5, 15, 19, 26, 32, 35, 38], [0, 2, 3, 4, 11, 16, 23, 25, 31, 36, 46], [0, 2, 2, 2, 12, 22, 24, 34, 37, 48], [1, 2, 6, 9, 30, 39, 45, 46], [2, 2, 7, 13, 14, 17, 24, 27, 28, 31, 44]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAStMbW56LXA"
      },
      "source": [
        "# **Latent Dirichlet Allocation**\r\n",
        "\r\n",
        "LDA is a statistical model that reflects the belonging of a document to several topics.\r\n",
        "\r\n",
        "We are usig CompleteDirichlet instead of Dirichlet because of its property to assign to the last element the rest of the sum to one.\r\n",
        "\r\n",
        "Alpha and beta reprezents the priors and are initialized with one.\r\n",
        "\r\n",
        "Let t be the number of topics and  d the size of the corpus. Thus, the LDA generative process is:\r\n",
        "\r\n",
        "1. For each topic: \r\n",
        "\r\n",
        " a) Draw a distribution over words $\\phi_d = Dirichlet(\\beta)$\r\n",
        "\r\n",
        "\r\n",
        "2. For each document: \r\n",
        "\r\n",
        " a) Draw a topic of vector proportions $\\theta_t = Dirichlet(\\alpha)$\r\n",
        "        \r\n",
        " b) For each word: \r\n",
        "\r\n",
        "    i) Draw a topic assignment $z_{d, t} = Multinomial(\\theta_d)$\r\n",
        "        \r\n",
        "    ii) Draw a word $w_{d, t} = Multinomial(\\phi_{z_{t, d}})$\r\n",
        "  \r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nteP24iB7Gy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d573ee6-c8bc-4004-85f7-606296d9ab5f"
      },
      "source": [
        "!pip install pymc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymc in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrSpaYHiHkat",
        "outputId": "9666aafc-76f7-46e9-8c70-22a82ac2048d"
      },
      "source": [
        "import pymc as pm\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "nr_topics = 2  \r\n",
        "vocab_size = len(vocab_)\r\n",
        "corpus_size = len(data)\r\n",
        "\r\n",
        "alpha = np.ones(nr_topics)*0.5\r\n",
        "beta = np.ones(vocab_size)*0.5\r\n",
        "Nm = [len(doc) for doc in data]\r\n",
        "\r\n",
        "phi_ = pm.Container([pm.Dirichlet(\"phi_ %s\" % topic, theta = beta) for topic in range(nr_topics)])\r\n",
        "\r\n",
        "phi = pm.Container( [pm.CompletedDirichlet(\"Phi %s\" % topic,  phi_[topic])  for topic in range(nr_topics)] ) #word distribution per topic\r\n",
        "\r\n",
        "theta_ = pm.Container([pm.Dirichlet(\"theta_ %s\" % doc, theta = alpha) for doc in range(corpus_size)])\r\n",
        "\r\n",
        "theta = pm.Container([pm.CompletedDirichlet(\"Theta %s\" % doc, theta_[doc]) for doc in range(corpus_size)])    # topic distribution per docs\r\n",
        "\r\n",
        "\r\n",
        "z = pm.Container([pm.Categorical('Z %i' % doc,       # topic for word per docs\r\n",
        "                             p = theta[doc], \r\n",
        "                             size = Nm[doc],\r\n",
        "                             value = np.random.randint(nr_topics, size = Nm[doc]))\r\n",
        "                for doc in range(corpus_size)])\r\n",
        "\r\n",
        "\r\n",
        "w = pm.Container([pm.Categorical(\"W %i %i\" % (doc, word),     # the word from doc\r\n",
        "                                p = pm.Lambda('Phi Z %i %i' % (doc, word), \r\n",
        "                                             lambda z = z[doc][word], \r\n",
        "                                             phi = phi: phi[z]),\r\n",
        "                                value = data[doc][word], \r\n",
        "                                observed = True)\r\n",
        "                for doc in range(corpus_size)\r\n",
        "                for word in range(Nm[doc]) ])\r\n",
        "\r\n",
        "\r\n",
        "model = pm.Model([phi_, theta_, theta, phi, z, w])\r\n",
        "\r\n",
        "map_ = pm.MAP(model) # improving convergence\r\n",
        "map_.fit()\r\n",
        "\r\n",
        "mcmc = pm.MCMC(model)    # fitting\r\n",
        "tr = mcmc.sample(1000, 400)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: Stochastic Z 7's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 2's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 3's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 4's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 1's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 5's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 0's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 6's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " [-----------------100%-----------------] 1000 of 1000 complete in 1.3 sec"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWrkm4fXyjBr",
        "outputId": "ec844bc1-cc80-4902-b0e6-b1054275ed17"
      },
      "source": [
        "print('Topic distribution for each word:\\n')\r\n",
        "for doc in range(corpus_size):  # topic distribution per word per document\r\n",
        "    print(mcmc.trace('Z %i' % doc)[-1]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic distribution for each word:\n",
            "\n",
            "[0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0]\n",
            "[1 1 1 1 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 1 1 1]\n",
            "[0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWVZ93gyDa4j",
        "outputId": "2cf3336e-240e-4c05-fbcd-fedfb35911c7"
      },
      "source": [
        "print('Topic distribution per document:\\n')\r\n",
        "for doc in range(corpus_size):  # topic distribution per document\r\n",
        "    print(mcmc.trace('Theta %i' % doc)[0]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic distribution per document:\n",
            "\n",
            "[[1.000000e+00 1.538214e-12]]\n",
            "[[1.00000000e+00 7.58637597e-12]]\n",
            "[[1.00000000e+00 7.94231347e-12]]\n",
            "[[1.00000000e+00 1.14478427e-11]]\n",
            "[[1.00000000e+00 6.23534557e-12]]\n",
            "[[2.92530819e-11 1.00000000e+00]]\n",
            "[[1.01291382e-11 1.00000000e+00]]\n",
            "[[1.00000000e+00 1.27328148e-11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_3_FTLEizgB"
      },
      "source": [
        "def topics__(nr_topics, vocab_, w):\r\n",
        "    \r\n",
        "    for topic in range(nr_topics):\r\n",
        "        print (\"Topic % i\" % topic)\r\n",
        "        \r\n",
        "        idxs = np.argsort(mcmc.trace('Phi %i' % topic)[:].mean(axis=0)[0], axis = 0)   # trace - iid draws from the posterior \r\n",
        "        words_ = [vocab_[idx] for idx in idxs]\r\n",
        "\r\n",
        "        list_ = []\r\n",
        "\r\n",
        "        for i, j in enumerate(mcmc.trace('Phi %i' % topic)[:].mean(axis=0)[0]):\r\n",
        "            for id in idxs:\r\n",
        "                if id == i:\r\n",
        "                    list_.append(j)\r\n",
        "\r\n",
        "        for ix in idxs[w:0:-1]:\r\n",
        "            print(\"\\t\", words_[ix], \":\", list_[ix])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW6mhxQUWbUQ",
        "outputId": "d2be5737-842e-402f-f181-e419aec70b89"
      },
      "source": [
        "topics__(nr_topics, vocab_,50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic  0\n",
            "\t forward : 0.1114883735174976\n",
            "\t work : 0.0771856823585537\n",
            "\t divers : 0.07340999710800894\n",
            "\t compet : 0.07324545134127951\n",
            "\t branch : 0.07218832590328363\n",
            "\t rang : 0.07079286527958571\n",
            "\t visual : 0.0565536940896106\n",
            "\t classic : 0.05621288795361776\n",
            "\t histori : 0.053191279235019116\n",
            "\t like : 0.03949055419212663\n",
            "\t perform : 0.035381638423640366\n",
            "\t point : 0.03442343620222972\n",
            "\t theatr : 0.025470167424602508\n",
            "\t involv : 0.018386331474519128\n",
            "\t proffesion : 0.016295045891610006\n",
            "\t team : 0.016242178345574114\n",
            "\t creat : 0.01597529672229588\n",
            "\t paint : 0.015058085437278417\n",
            "\t architectur : 0.015016878264685122\n",
            "\t literatur : 0.01407624961973752\n",
            "\t shoot : 0.010549267409372731\n",
            "\t basketbal : 0.01039260322570474\n",
            "\t tall : 0.010174268239288629\n",
            "\t three : 0.009059318030847906\n",
            "\t game : 0.008860286229730075\n",
            "\t danc : 0.007932262103898852\n",
            "\t activ : 0.007437828541845352\n",
            "\t player : 0.005973253468021714\n",
            "\t human : 0.005684990361801037\n",
            "\t art : 0.005610168500504996\n",
            "\t film : 0.005040808537219693\n",
            "\t music : 0.004264190053252695\n",
            "\t friend : 0.002760671927697135\n",
            "\t denni : 0.0025567470802901325\n",
            "\t object : 0.002458424820386719\n",
            "\t center : 0.0022199233720498366\n",
            "\t rodman : 0.0015756169859316642\n",
            "\t sport : 0.001366941355766171\n",
            "\t product : 0.001256854454220295\n",
            "\t sculptur : 0.0010364829865923575\n",
            "\t includ : 0.0010045199450308807\n",
            "\t broader : 0.0007940268941913243\n",
            "\t critic : 0.0006081260203131264\n",
            "\t win : 0.0004707365812858984\n",
            "\t relat : 0.0003995141934443823\n",
            "\t definit : 0.0003062185933003132\n",
            "\t general : 0.00010956931705509422\n",
            "\t audit : 1.1961983154432448e-05\n",
            "Topic  1\n",
            "\t art : 0.11591236971001206\n",
            "\t paint : 0.11280819207586172\n",
            "\t general : 0.10318398707834402\n",
            "\t audit : 0.09963500849633679\n",
            "\t includ : 0.07751612764524604\n",
            "\t work : 0.05771346042106207\n",
            "\t perform : 0.053495228391525144\n",
            "\t object : 0.04205134995448034\n",
            "\t point : 0.032694353696925214\n",
            "\t denni : 0.03184138655610979\n",
            "\t artifact : 0.029310605401554893\n",
            "\t human : 0.02760808852559487\n",
            "\t tall : 0.021419923561106554\n",
            "\t involv : 0.019715267845883986\n",
            "\t game : 0.019413031918546642\n",
            "\t definit : 0.018449849102580497\n",
            "\t critic : 0.01598911650100969\n",
            "\t relat : 0.01262519028888124\n",
            "\t sculptur : 0.011917306194177139\n",
            "\t literatur : 0.010991404232183683\n",
            "\t broader : 0.010017534658794963\n",
            "\t team : 0.009336579096277832\n",
            "\t classic : 0.00925267887072234\n",
            "\t divers : 0.008410316773825772\n",
            "\t shoot : 0.007108017091085236\n",
            "\t rodman : 0.006290343211507455\n",
            "\t basketbal : 0.005280753560451351\n",
            "\t music : 0.0044004740597353\n",
            "\t like : 0.0043086217558796595\n",
            "\t player : 0.003618249327591763\n",
            "\t activ : 0.003433430947083139\n",
            "\t danc : 0.003219895269802611\n",
            "\t branch : 0.002453988198423569\n",
            "\t compet : 0.00178227629007765\n",
            "\t theatr : 0.001698800266324069\n",
            "\t center : 0.001677697817208034\n",
            "\t architectur : 0.001666076103740277\n",
            "\t rang : 0.0011882881328373724\n",
            "\t histori : 0.00019157921065625336\n",
            "\t win : 0.00012620226838311866\n",
            "\t visual : 9.854238510018643e-05\n",
            "\t film : 5.911330820253552e-05\n",
            "\t forward : 2.7882905354168416e-05\n",
            "\t proffesion : 2.7122424858517364e-05\n",
            "\t sport : 1.5031958805242181e-05\n",
            "\t friend : 1.0502055410700224e-05\n",
            "\t product : 4.433274577558038e-06\n",
            "\t three : 4.32117803974381e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgQOobgRMrwC"
      },
      "source": [
        "# **TASK2**\r\n",
        "\r\n",
        "# Can the topic model be used to define a topic-based similarity measure between documents?\r\n",
        "\r\n",
        "We will analyze the similariy between topics by computing the cosine distance and JensenShannon distance of the theta distribution (i.e. the distribution of topics per document) of the first n documents given"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAN2MGUqIUMc"
      },
      "source": [
        "from scipy.spatial import distance\r\n",
        "\r\n",
        "def cos_sim(nr_doc):\r\n",
        "\r\n",
        "    print('Documents \\t cosine similarity \\t JensenShannon similarity')\r\n",
        "    for i in range(nr_doc):\r\n",
        "        for j in range(nr_doc):\r\n",
        "            if i != j:\r\n",
        "                print('%i & %i:\\t\\t %f \\t\\t %f ' %(i,j,\r\n",
        "                                             1-distance.cosine( mcmc.trace('Theta %i' % i)[:].mean(axis=0)[0] , mcmc.trace('Theta %i' % j)[:].mean(axis=0)[0]),\r\n",
        "                                             1-distance.jensenshannon( mcmc.trace('Theta %i' % i)[:].mean(axis=0)[0] , mcmc.trace('Theta %i' % j)[:].mean(axis=0)[0])))       \r\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfJQHQt-LKDI",
        "outputId": "c989aabd-4d5c-435b-de34-175a3691dbab"
      },
      "source": [
        "print('Topic similarity between documents:\\n')\r\n",
        "cos_sim(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic similarity between documents:\n",
            "\n",
            "Documents \t cosine similarity \t JensenShannon similarity\n",
            "0 & 1:\t\t 1.000000 \t\t 0.999999 \n",
            "0 & 2:\t\t 1.000000 \t\t 0.999999 \n",
            "0 & 3:\t\t 1.000000 \t\t 0.999999 \n",
            "0 & 4:\t\t 1.000000 \t\t 0.999999 \n",
            "0 & 5:\t\t 0.000000 \t\t 0.167445 \n",
            "1 & 0:\t\t 1.000000 \t\t 0.999999 \n",
            "1 & 2:\t\t 1.000000 \t\t 1.000000 \n",
            "1 & 3:\t\t 1.000000 \t\t 1.000000 \n",
            "1 & 4:\t\t 1.000000 \t\t 1.000000 \n",
            "1 & 5:\t\t 0.000000 \t\t 0.167445 \n",
            "2 & 0:\t\t 1.000000 \t\t 0.999999 \n",
            "2 & 1:\t\t 1.000000 \t\t 1.000000 \n",
            "2 & 3:\t\t 1.000000 \t\t 1.000000 \n",
            "2 & 4:\t\t 1.000000 \t\t 1.000000 \n",
            "2 & 5:\t\t 0.000000 \t\t 0.167445 \n",
            "3 & 0:\t\t 1.000000 \t\t 0.999999 \n",
            "3 & 1:\t\t 1.000000 \t\t 1.000000 \n",
            "3 & 2:\t\t 1.000000 \t\t 1.000000 \n",
            "3 & 4:\t\t 1.000000 \t\t 0.999999 \n",
            "3 & 5:\t\t 0.000000 \t\t 0.167445 \n",
            "4 & 0:\t\t 1.000000 \t\t 0.999999 \n",
            "4 & 1:\t\t 1.000000 \t\t 1.000000 \n",
            "4 & 2:\t\t 1.000000 \t\t 1.000000 \n",
            "4 & 3:\t\t 1.000000 \t\t 0.999999 \n",
            "4 & 5:\t\t 0.000000 \t\t 0.167445 \n",
            "5 & 0:\t\t 0.000000 \t\t 0.167445 \n",
            "5 & 1:\t\t 0.000000 \t\t 0.167445 \n",
            "5 & 2:\t\t 0.000000 \t\t 0.167445 \n",
            "5 & 3:\t\t 0.000000 \t\t 0.167445 \n",
            "5 & 4:\t\t 0.000000 \t\t 0.167445 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMRrcN9pqxtN"
      },
      "source": [
        "# **What about a new document? How can topics be assigned to it?**\r\n",
        "\r\n",
        "If we are given a new document, we can compare the probability of the document belonging to topic 1 with the probability of the document belonging to topic 2 using Bayes Theorem.\r\n",
        "\r\n",
        "Thus, we want to compute $F = \\frac{P(\\theta_1|NewD)}{P(\\theta_2|NewD)}$, knowing that $ P(\\phi_i|NewD) = \\frac{P(NewD|\\theta_i)P(\\theta_i)}{P(NewD)} = \\frac{ \\prod_{j} P(NewD_j|\\theta_i)P(\\theta_i)}{P(NewD)} $. \r\n",
        "\r\n",
        "Therefor, $F = \\frac{ \\prod_{j} P(NewD_j|\\theta_1)P(\\theta_1)}{\\prod_{j} P(NewD_j|\\theta_2)P(\\theta_2)}$.\r\n",
        "\r\n",
        "$P(\\theta_i) = $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMp4CUi6whzf"
      },
      "source": [
        "# Computing P(Ti)\r\n",
        "pti = []\r\n",
        "for topic in range(nr_topics):\r\n",
        "    pp = 1\r\n",
        "    for j in mcmc.trace('Phi %i' % topic)[:].mean(axis=0)[0]:\r\n",
        "        pp = pp*j\r\n",
        "    \r\n",
        "    pti.append(pp)\r\n",
        "        \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCX6_1cR28Nv",
        "outputId": "a983c2a0-aea8-4b54-c4af-bf3c1f4f5661"
      },
      "source": [
        "print(pti)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.157017892853357e-117, 8.602118166862097e-131]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvTwpEoc4uWe"
      },
      "source": [
        "NewD = ['Basketball player tall art dance']\r\n",
        "\r\n",
        "dataN, data_N = preprocessing(NewD)\r\n",
        "vocN, occN = feat_extraction(dataN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy8_3IbMBa68",
        "outputId": "8bd86657-350b-46e4-f8cf-55b7ba48ca7e"
      },
      "source": [
        "dataN, data_occN = frec(occN)\r\n",
        "print(dataN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0, 0.02040816326530612, 0.04081632653061224, 0.061224489795918366, 0.08163265306122448]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV4KszWSDPqd"
      },
      "source": [
        "vocab_N = {v : k for k, v in vocN.vocabulary_.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_YZztUXEABg",
        "outputId": "4f248f64-a56b-45bf-b3af-070efae72b9d"
      },
      "source": [
        "print(vocab_N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'basketbal', 3: 'player', 4: 'tall', 0: 'art', 2: 'danc'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwzapyNxFQRd"
      },
      "source": [
        "basketbal0 = 0.01039260322570474\r\n",
        "tall0 = 0.010174268239288629\r\n",
        "danc0 = 0.007932262103898852\r\n",
        "player0 = 0.005973253468021714\r\n",
        "art0 = 0.005610168500504996\r\n",
        "\r\n",
        "art1 = 0.11591236971001206\r\n",
        "basketbal1 = 0.005280753560451351\r\n",
        "player1 = 0.003618249327591763\r\n",
        "danc1 = 0.003219895269802611\r\n",
        "tall1 = 0.021419923561106554"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4_3i7VpBzGK"
      },
      "source": [
        "p_newd_t0 = basketbal0*art0*danc0*player0*tall0*pti[0]\r\n",
        "p_newd_t1 = basketbal1*art1*danc1*player1*tall1*pti[1]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7cCGUWkHZIL",
        "outputId": "756252f9-2af2-44c8-b7c7-57d7295e3a16"
      },
      "source": [
        "if p_newd_t0 > p_newd_t1:\r\n",
        "    print('NewD has topic 0 ')\r\n",
        "else:\r\n",
        "    print('NewD has topic 1 ')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NewD has topic 0 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}