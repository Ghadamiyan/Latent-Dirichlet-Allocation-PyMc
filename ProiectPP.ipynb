{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProiectPP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCfro7jDPPyD"
      },
      "source": [
        "# **Probabilistic Programming Project**\r\n",
        "\r\n",
        "Ghadamiyan Lida\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "The purpose of this project is to implement the LDA algorithm using PyMC, following the indications from the course.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycEAQ0FEnHON"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "corpus = [\"I had a peanuts butter sandwich for breakfast\", \r\n",
        "          \"I like to eat almonds, peanuts and walnuts\", \r\n",
        "          \"My neighbour got a little dog yesterday\",\r\n",
        "          \"Cats and dogs are mortal enemies\",\r\n",
        "          \"You mustn't feed peanuts to your dog\"]\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7eObDgj3tWJ"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQq270h5rB1e"
      },
      "source": [
        "##**Data Preprocessing**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "* Tokenization. \r\n",
        "* Stopwords removal - done by CountVectorizer in feature extraction. \r\n",
        "* Lowercase the words. \r\n",
        "* Remove punctuation.\r\n",
        "* Remove short words.\r\n",
        "* Lematization - reducing words to their meaningful base form.\r\n",
        "* Stemming â€” reducing words to their base form.\r\n",
        "* **Feature Extraction**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeJ31PeZnZLu",
        "outputId": "8bd64d1c-3f2c-4891-faa7-dc490304a8f2"
      },
      "source": [
        "import string\r\n",
        "import nltk\r\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "stemmer = SnowballStemmer('english')\r\n",
        "\r\n",
        "def preprocessing(corpus):\r\n",
        "\r\n",
        "    df = pd.DataFrame({'doc':corpus})     # Converting to data frame\r\n",
        "\r\n",
        "    data = []\r\n",
        "    for i in range(0, len(df.index)):\r\n",
        "\r\n",
        "        table = str.maketrans(dict.fromkeys(string.punctuation))                    # Punctuation removal\r\n",
        "        words = (df.doc[i].translate(table)).lower() \r\n",
        "\r\n",
        "        words = nltk.word_tokenize(words) # Tokenization\r\n",
        "\r\n",
        "        words_ = []\r\n",
        "        for word in words:\r\n",
        "            if len(word) > 2:                                                       # Short words removal\r\n",
        "                word1 = stemmer.stem(WordNetLemmatizer().lemmatize(word, pos='v'))  # Lemmatization and stemming           \r\n",
        "                words_.append(word1)\r\n",
        "        data.append(words_)\r\n",
        "\r\n",
        "    #print(data) # list of list\r\n",
        "    corpus = pd.DataFrame({'doc':data})                                             # Saving as dataframe\r\n",
        "    return corpus, data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmL59peycDHh",
        "outputId": "0f06b347-954d-428a-f37c-afe9764a067f"
      },
      "source": [
        "data, data_ = preprocessing(corpus)\r\n",
        "print([len(doc) for doc in data_])\r\n",
        "print(len(data))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6, 6, 5, 6, 6]\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndWqUlOfPfCd"
      },
      "source": [
        "# **Feature Extraction**\r\n",
        "\r\n",
        "* Bag of Words - CountVectorizer is used for building a dictionary of features, and also for tokenizing and filtering stopwords. \r\n",
        "* Frequencies - TfidfTransformer compute the term frequencies by  dividing the number of occurrences given by CountVectorizer by the total number of words.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQw0wC7WRCYN"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "\r\n",
        "def feat_extraction(data):\r\n",
        "    data = data['doc'].astype(str).values.tolist() # convert df to list of strings so that tfidf would work\r\n",
        "    #print(data)\r\n",
        "    \r\n",
        "    count_vect =  CountVectorizer(lowercase='false', stop_words='english')\r\n",
        "    occurrence = count_vect.fit_transform(data)\r\n",
        "\r\n",
        "    tf_transformer = TfidfTransformer(use_idf=False).fit(occurrence)\r\n",
        "    tf_vect = tf_transformer.transform(occurrence)\r\n",
        "\r\n",
        "    return count_vect, occurrence"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oqnKxZXdCng",
        "outputId": "2ec42c30-81a6-44e0-81dc-49a123a528b4"
      },
      "source": [
        "voc, occ = feat_extraction(data)\r\n",
        "\r\n",
        "#print(voc.get_feature_names())\r\n",
        "#print(voc.vocabulary_)\r\n",
        "\r\n",
        "vocab_ = {v : k for k, v in voc.vocabulary_.items()}\r\n",
        "print(vocab_)\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{13: 'peanut', 2: 'butter', 14: 'sandwich', 1: 'breakfast', 8: 'like', 5: 'eat', 0: 'almond', 15: 'walnut', 12: 'neighbour', 9: 'littl', 4: 'dog', 16: 'yesterday', 3: 'cat', 10: 'mortal', 6: 'enemi', 11: 'mustnt', 7: 'fee'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsndF5APqfhh",
        "outputId": "8649b1b4-8d01-492a-ff31-4e007a54d43d"
      },
      "source": [
        "data_occ = []\r\n",
        "for doc in occ.toarray():\r\n",
        "    words = []\r\n",
        "    for poz, n in enumerate(doc):\r\n",
        "        if n == 1:\r\n",
        "            words.append(poz)\r\n",
        "    data_occ.append(words)\r\n",
        "\r\n",
        "print(data_occ)\r\n",
        "\r\n",
        "\r\n",
        "data_tf = []\r\n",
        "for doc in data_occ:\r\n",
        "    doc_tf = []\r\n",
        "    for w in doc:\r\n",
        "        doc_tf.append(w / len(vocab_))\r\n",
        "    data_tf.append(doc_tf)\r\n",
        "\r\n",
        "print(data_tf)\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 13, 14], [0, 5, 8, 13, 15], [4, 9, 12, 16], [3, 4, 6, 10], [4, 7, 11, 13]]\n",
            "[[0.058823529411764705, 0.11764705882352941, 0.7647058823529411, 0.8235294117647058], [0.0, 0.29411764705882354, 0.47058823529411764, 0.7647058823529411, 0.8823529411764706], [0.23529411764705882, 0.5294117647058824, 0.7058823529411765, 0.9411764705882353], [0.17647058823529413, 0.23529411764705882, 0.35294117647058826, 0.5882352941176471], [0.23529411764705882, 0.4117647058823529, 0.6470588235294118, 0.7647058823529411]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwi1Mv7_ytW-"
      },
      "source": [
        "After I tried both with data and data_tf, I concluded that working with frequencies indead of occurences gives better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3LUzwNlu9j1"
      },
      "source": [
        "data = data_tf"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b95olbfFWkHg"
      },
      "source": [
        "# **Latent Dirichlet Allocation**\r\n",
        "\r\n",
        "LDA is a statistical model that reflects the belonging of a document to several topics.\r\n",
        "\r\n",
        "We are usig CompleteDirichlet instead of Dirichlet because of its property to assign to the last element the rest of the sum to one.\r\n",
        "\r\n",
        "Alpha and beta reprezents the priors and are initialized with one.\r\n",
        "\r\n",
        "Let t be the number of topics and  d the size of the corpus. Thus, the LDA generative process is:\r\n",
        "\r\n",
        "1. For each topic: \r\n",
        "\r\n",
        " a) Draw a distribution over words $\\phi_d = Dirichlet(\\beta)$\r\n",
        "\r\n",
        "\r\n",
        "2. For each document: \r\n",
        "\r\n",
        " a) Draw a topic of vector proportions $\\theta_t = Dirichlet(\\alpha)$\r\n",
        "        \r\n",
        " b) For each word: \r\n",
        "\r\n",
        "    i) Draw a topic assignment $z_{d, t} = Multinomial(\\theta_d)$\r\n",
        "        \r\n",
        "    ii) Draw a word $w_{d, t} = Multinomial(\\phi_{z_{t, d}})$\r\n",
        "  \r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MStZBuny3v0x",
        "outputId": "7277f11f-b3d9-4a65-b5a0-606a46ab8961"
      },
      "source": [
        "!pip install pymc"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymc in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ2Rn-n96BNi",
        "outputId": "f3e13329-ec70-4bcb-bbf9-7961fa33a0b0"
      },
      "source": [
        "import pymc as pm\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "nr_topics = 2  \r\n",
        "vocab_size = len(vocab_)\r\n",
        "corpus_size = len(data)\r\n",
        "\r\n",
        "alpha = np.ones(nr_topics)*0.5\r\n",
        "beta = np.ones(vocab_size)*0.5\r\n",
        "Nm = [len(doc) for doc in data]\r\n",
        "\r\n",
        "phi_ = pm.Container([pm.Dirichlet(\"phi_ %s\" % topic, theta = beta) for topic in range(nr_topics)])\r\n",
        "\r\n",
        "phi = pm.Container( [pm.CompletedDirichlet(\"Phi %s\" % topic,  phi_[topic])  for topic in range(nr_topics)] ) #word distribution per topic\r\n",
        "\r\n",
        "theta_ = pm.Container([pm.Dirichlet(\"theta_ %s\" % doc, theta = alpha) for doc in range(corpus_size)])\r\n",
        "\r\n",
        "theta = pm.Container([pm.CompletedDirichlet(\"Theta %s\" % doc, theta_[doc]) for doc in range(corpus_size)])    # topic distribution per docs\r\n",
        "\r\n",
        "\r\n",
        "z = pm.Container([pm.Categorical('Z %i' % doc,       # topic for word per docs\r\n",
        "                             p = theta[doc], \r\n",
        "                             size = Nm[doc],\r\n",
        "                             value = np.random.randint(nr_topics, size = Nm[doc]))\r\n",
        "                for doc in range(corpus_size)])\r\n",
        "\r\n",
        "\r\n",
        "w = pm.Container([pm.Categorical(\"W %i %i\" % (doc, word),     # the word from doc\r\n",
        "                                p = pm.Lambda('Phi Z %i %i' % (doc, word), \r\n",
        "                                             lambda z = z[doc][word], \r\n",
        "                                             phi = phi: phi[z]),\r\n",
        "                                value = data[doc][word], \r\n",
        "                                observed = True)\r\n",
        "                for doc in range(corpus_size)\r\n",
        "                for word in range(Nm[doc]) ])\r\n",
        "\r\n",
        "\r\n",
        "model = pm.Model([phi_, theta_, theta, phi, z, w])\r\n",
        "\r\n",
        "map_ = pm.MAP(model) # improving convergence\r\n",
        "map_.fit()\r\n",
        "\r\n",
        "mcmc = pm.MCMC(model)    # fitting\r\n",
        "tr = mcmc.sample(10000, 4000)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: Stochastic Z 0's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 2's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 3's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 4's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n",
            "Warning: Stochastic Z 1's value is neither numerical nor array with floating-point dtype. Recommend fitting method fmin (default).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
            "  warnings.warn(message)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " [-----------------100%-----------------] 10000 of 10000 complete in 16.0 sec"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNiQRs6Mw9iQ",
        "outputId": "34115484-6604-4bb3-bcc2-28538dffee20"
      },
      "source": [
        "print('Topic distribution for each word:\\n')\r\n",
        "for doc in range(corpus_size):  # topic distribution per word per document\r\n",
        "    print(mcmc.trace('Z %i' % doc)[-1]) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic distribution for each word:\n",
            "\n",
            "[1 1 1 1]\n",
            "[1 1 1 1 1]\n",
            "[0 0 0 0]\n",
            "[0 0 0 0]\n",
            "[0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI7h33Re10Ib"
      },
      "source": [
        "Topic distribution for each word: -------------- data:\r\n",
        "\r\n",
        "[1 1 1 1] ----------------------------------------------------- [peanut, butter, sandwich, for, breakfast]\r\n",
        "\r\n",
        "[1 1 1 1 1] -------------------------------------------------- [like, eat, almond, peanut, and, walnut]\r\n",
        "\r\n",
        "[0 0 0 0] ----------------------------------------------------- [neighbour, get, littl, dog, yesterday]\r\n",
        "\r\n",
        "[0 0 0 0] ----------------------------------------------------- [cat, and, dog, be, mortal, enemi]\r\n",
        "\r\n",
        "[0 0 0 0] ----------------------------------------------------- [you, mustnt, fee, peanut, your, dog]\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUF7FL4Z2cmr"
      },
      "source": [
        "def topics__(nr_topics, vocab_, w):\r\n",
        "    \r\n",
        "    for topic in range(nr_topics):\r\n",
        "        print (\"Topic % i\" % topic)\r\n",
        "        \r\n",
        "        idxs = np.argsort(mcmc.trace('Phi %i' % topic)[:].mean(axis=0)[0], axis = 0)   # trace - iid draws from the posterior \r\n",
        "        words_ = [vocab_[idx] for idx in idxs]\r\n",
        "\r\n",
        "        list_ = []\r\n",
        "\r\n",
        "        for i, j in enumerate(mcmc.trace('Phi %i' % topic)[:].mean(axis=0)[0]):\r\n",
        "            for id in idxs:\r\n",
        "                if id == i:\r\n",
        "                    list_.append(j)\r\n",
        "\r\n",
        "        for ix in idxs[w:0:-1]:\r\n",
        "            print(\"\\t\", words_[ix], \":\", list_[ix])\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDJEs6B72iMM",
        "outputId": "65c00593-cae9-4470-967e-07ad40604ec4"
      },
      "source": [
        "print('Words distribution per topic:\\n')\r\n",
        "topics__(nr_topics, vocab_, 5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words distribution per topic:\n",
            "\n",
            "Topic  0\n",
            "\t neighbour : 0.007115568368636087\n",
            "\t dog : 0.003947428131491523\n",
            "\t breakfast : 0.0035652383691614177\n",
            "\t walnut : 0.0021047412664710404\n",
            "\t sandwich : 0.001380059487851138\n",
            "Topic  1\n",
            "\t enemi : 0.011000210173604476\n",
            "\t yesterday : 0.01062986523847483\n",
            "\t breakfast : 0.002811108860692383\n",
            "\t peanut : 0.002308913654492566\n",
            "\t mustnt : 0.00032400491911813536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPNg5uJiHdGA"
      },
      "source": [
        "Words distribution per topic:\r\n",
        "\r\n",
        "Topic  0\r\n",
        "\r\n",
        "\t neighbour : 0.007115568368636087\r\n",
        "\t dog : 0.003947428131491523\r\n",
        "\t breakfast : 0.0035652383691614177\r\n",
        "\t walnut : 0.0021047412664710404\r\n",
        "\t sandwich : 0.001380059487851138\r\n",
        "     \r\n",
        "Topic  1\r\n",
        "\r\n",
        "\t enemi : 0.011000210173604476\r\n",
        "\t yesterday : 0.01062986523847483\r\n",
        "\t breakfast : 0.002811108860692383\r\n",
        "\t peanut : 0.002308913654492566\r\n",
        "\t mustnt : 0.00032400491911813536"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzIKgdcEkNz",
        "outputId": "53a0648a-93ea-4516-8b75-7dcfc7aebc54"
      },
      "source": [
        "print('Topic distribution per document:\\n')\r\n",
        "for doc in range(corpus_size):  # topic distribution document\r\n",
        "    print(mcmc.trace('Theta %i' % doc)[0]) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic distribution per document:\n",
            "\n",
            "[[8.72466055e-10 9.99999999e-01]]\n",
            "[[5.48124998e-10 9.99999999e-01]]\n",
            "[[1.00000000e+00 6.36080077e-12]]\n",
            "[[1.00000000e+00 4.47131221e-12]]\n",
            "[[1.00000000e+00 8.57880433e-12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSJ-_5R7HWxg"
      },
      "source": [
        "Topic distribution per document:\r\n",
        "\r\n",
        "[[8.72466055e-10 9.99999999e-01]]\r\n",
        "\r\n",
        "[[5.48124998e-10 9.99999999e-01]]\r\n",
        "\r\n",
        "[[1.00000000e+00 6.36080077e-12]]\r\n",
        "\r\n",
        "[[1.00000000e+00 4.47131221e-12]]\r\n",
        "\r\n",
        "[[1.00000000e+00 8.57880433e-12]]\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN8AMUl2NnQ1"
      },
      "source": [
        "# **TASK2**\r\n",
        "\r\n",
        "# Can the topic model be used to define a topic-based similarity measure between documents?\r\n",
        "\r\n",
        "We will analyze the similariy between topics by computing the cosine distance and JensenShannon distance of the theta distribution (i.e. the distribution of topics per document) of the first n documents given.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVB7O2SMS_9"
      },
      "source": [
        "from scipy.spatial import distance\r\n",
        "\r\n",
        "def cos_sim(nr_doc):\r\n",
        "\r\n",
        "    print('Documents \\t cosine similarity \\t JensenShannon simlarity')\r\n",
        "    for i in range(nr_doc):\r\n",
        "        for j in range(nr_doc):\r\n",
        "            if i != j:\r\n",
        "                print('%i & %i:\\t\\t %f \\t\\t %f ' %(i,j,\r\n",
        "                                             1-distance.cosine( mcmc.trace('Theta %i' % i)[:].mean(axis=0)[0] , mcmc.trace('Theta %i' % j)[:].mean(axis=0)[0]),\r\n",
        "                                             1-distance.jensenshannon( mcmc.trace('Theta %i' % i)[:].mean(axis=0)[0] , mcmc.trace('Theta %i' % j)[:].mean(axis=0)[0])))       \r\n",
        "                "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a1sgdDCMZke",
        "outputId": "5456b9aa-8939-4a9d-a57a-e925822fc11a"
      },
      "source": [
        "print('Topic similarity between documents:\\n')\r\n",
        "cos_sim(3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic similarity between documents:\n",
            "\n",
            "Documents \t cosine similarity \t JensenShannon simlarity\n",
            "0 & 1:\t\t 1.000000 \t\t 0.999985 \n",
            "0 & 2:\t\t 0.000155 \t\t 0.167900 \n",
            "1 & 0:\t\t 1.000000 \t\t 0.999985 \n",
            "1 & 2:\t\t 0.000155 \t\t 0.167900 \n",
            "2 & 0:\t\t 0.000155 \t\t 0.167900 \n",
            "2 & 1:\t\t 0.000155 \t\t 0.167900 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}